{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271c77b3",
   "metadata": {},
   "source": [
    "Evaluaci√≥n del m√≥dulo\n",
    "Proyecto: Preparaci√≥n de datos\n",
    "Situaci√≥n inicial üìç\n",
    "Unidad solicitante: Equipo de Anal√≠tica de Datos de una empresa de e-commerce\n",
    "\n",
    "\n",
    "üìå El equipo de anal√≠tica de datos ha recibido la solicitud de preparar y\n",
    "estructurar un conjunto de datos provenientes de diversas fuentes (archivos CSV,\n",
    "Excel y p√°ginas web) para su posterior an√°lisis. Actualmente, los datos presentan\n",
    "m√∫ltiples problemas: valores perdidos, registros duplicados, formatos\n",
    "inconsistentes y presencia de outliers que distorsionan los resultados.\n",
    "La problem√°tica a resolver consiste en transformar y limpiar estos datos,\n",
    "garantizando su calidad y estructuraci√≥n para que puedan ser utilizados en\n",
    "futuros modelos predictivos y reportes de negocio. La gerencia de datos ya ha\n",
    "definido las herramientas a utilizar y solicita la implementaci√≥n t√©cnica de un flujo\n",
    "de trabajo integral para el tratamiento de los datos.\n",
    "\n",
    "\n",
    "Nuestro objetivo üìã\n",
    "\n",
    "El objetivo principal del proyecto es desarrollar un proceso automatizado y\n",
    "eficiente para la obtenci√≥n, limpieza, transformaci√≥n, an√°lisis y estructuraci√≥n\n",
    "de datos utilizando Python y las librer√≠as NumPy y Pandas.\n",
    "Al finalizar el proyecto, se espera contar con un dataset limpio, confiable y\n",
    "estructurado, listo para ser utilizado en procesos de an√°lisis y toma de decisiones\n",
    "en la organizaci√≥n.\n",
    "\n",
    "Este objetivo responde a la necesidad de la empresa de disponer de datos de\n",
    "calidad para la elaboraci√≥n de reportes estrat√©gicos y la implementaci√≥n de\n",
    "modelos de machine learning.\n",
    "\n",
    "\n",
    "\n",
    "Requerimientos\n",
    "\n",
    "Requerimientos generales:\n",
    "\n",
    "‚Üí Implementar un flujo de trabajo que incluya las etapas de carga, limpieza,\n",
    "transformaci√≥n y estructuraci√≥n de datos.\n",
    "\n",
    "‚Üí Utilizar exclusivamente las librer√≠as NumPy y Pandas para la manipulaci√≥n\n",
    "de los datos.\n",
    "\n",
    "‚Üí Documentar cada paso del proceso para asegurar la trazabilidad y\n",
    "comprensi√≥n del trabajo realizado.\n",
    "\n",
    "\n",
    "Requerimientos t√©cnicos espec√≠ficos:\n",
    "\n",
    "‚Üí Leer datos desde archivos CSV, Excel y p√°ginas web.\n",
    "\n",
    "‚Üí Identificar y gestionar valores nulos mediante imputaci√≥n o eliminaci√≥n.\n",
    "\n",
    "‚Üí Detectar y tratar outliers aplicando t√©cnicas estad√≠sticas.\n",
    "\n",
    "‚Üí Realizar tareas de Data Wrangling: ordenamiento, eliminaci√≥n de\n",
    "duplicados, reemplazo y transformaci√≥n de valores.\n",
    "\n",
    "‚Üí Aplicar agrupamiento y pivotado de datos utilizando funciones como\n",
    "groupby(), pivot() y melt().\n",
    "\n",
    "‚Üí Combinar m√∫ltiples fuentes de datos mediante merge() y concat().\n",
    "\n",
    "‚Üí Exportar el dataset final en formato CSV y Excel.\n",
    "\n",
    "‚Üí Incluir un script Python funcional y modularizado para ejecutar todo el\n",
    "proceso.\n",
    "\n",
    "\n",
    "Paso a paso üë£\n",
    "\n",
    "Este proyecto refiere exclusivamente al m√≥dulo 3: Obtenci√≥n y preparaci√≥n de\n",
    "datos, y se compone de 6 etapas (lecciones), las cuales podr√°s avanzar de forma\n",
    "progresiva y escalonada con la ayuda de los manuales te√≥ricos y los contenidos\n",
    "desarrollados en las clases en vivo.\n",
    "\n",
    "Ten en cuenta de invertir tiempo asincr√≥nicos para el desarrollo de cada etapa a\n",
    "modo de poder finalizar el m√≥dulo y realizar la entrega formal de tu propuesta.\n",
    "\n",
    "Cualquier consulta que surja comp√°rtela en los espacios sincr√≥nicos para resolver\n",
    "las dudas en equipo.\n",
    "\n",
    "A continuaci√≥n encontrar√°s las consignas y tareas a desarrollar:\n",
    "\n",
    "\n",
    "‚óè Lecci√≥n 1 - La librer√≠a numpy\n",
    "\n",
    "üéØ Objetivo: Crear un conjunto de datos ficticio utilizando NumPy,\n",
    "aplicando operaciones b√°sicas para la preparaci√≥n inicial.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Crear un archivo .py o un Notebook .ipynb.\n",
    "\n",
    "2. Generar datos ficticios de clientes y transacciones utilizando\n",
    "arrays de NumPy.\n",
    "\n",
    "3. Aplicar operaciones matem√°ticas b√°sicas (suma, media,\n",
    "conteo, etc.).\n",
    "\n",
    "4. Guardar los datos generados en un archivo .npy o convertirlos\n",
    "a listas para usarlos luego en Pandas.\n",
    "\n",
    "5. Explicar en un breve documento por qu√© NumPy es eficiente\n",
    "para el manejo de datos num√©ricos.\n",
    "\n",
    "\n",
    "‚Üí Nota: Este archivo servir√° de insumo para la siguiente lecci√≥n,\n",
    "donde estos datos ser√°n cargados y explorados con Pandas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "‚óè Lecci√≥n 2 - La librer√≠a pandas\n",
    "\n",
    "üéØ Objetivo: Explorar y transformar los datos generados en la Lecci√≥n\n",
    "1, utilizando la estructura de DataFrame de Pandas.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Leer los datos preparados en NumPy y convertirlos en un\n",
    "DataFrame.\n",
    "\n",
    "2. Realizar una exploraci√≥n inicial:\n",
    "\n",
    "‚óè Visualizar primeras y √∫ltimas filas.\n",
    "‚óè Obtener estad√≠sticas descriptivas.\n",
    "‚óè Aplicar filtros condicionales.\n",
    "\n",
    "\n",
    "3. Guardar el DataFrame preliminar en un archivo CSV para ser\n",
    "utilizado en la siguiente lecci√≥n.\n",
    "\n",
    "4. Redactar un documento breve describiendo los hallazgos y la\n",
    "utilidad de Pandas para la manipulaci√≥n de datos.\n",
    "\n",
    "\n",
    "\n",
    "‚óè Lecci√≥n 3 - Obtenci√≥n de datos desde archivos\n",
    "\n",
    "üéØ Objetivo: Integrar datos de diversas fuentes y unificarlos en un solo\n",
    "DataFrame para su posterior limpieza.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Cargar el archivo CSV generado en la Lecci√≥n 2.\n",
    "\n",
    "2. Incorporar nuevas fuentes de datos:\n",
    "\n",
    "‚óè Leer un archivo Excel con informaci√≥n complementaria.\n",
    "‚óè Extraer datos de una tabla web usando read_html().\n",
    "\n",
    "3. Unificar las diferentes fuentes de datos en un √∫nico\n",
    "DataFrame.\n",
    "\n",
    "4. Guardar el DataFrame consolidado y documentar los desaf√≠os\n",
    "encontrados al obtener datos de distintos formatos.\n",
    "\n",
    "‚Üí Nota: Este DataFrame unificado ser√° la base para realizar la\n",
    "limpieza y transformaci√≥n en las siguientes etapas.\n",
    "\n",
    "\n",
    "\n",
    "‚óè Lecci√≥n 4 - Manejo de valores perdidos y outliers\n",
    "\n",
    "üéØ Objetivo: Aplicar t√©cnicas de limpieza de datos, resolviendo\n",
    "problemas de valores nulos y datos at√≠picos.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Identificar valores nulos en el DataFrame consolidado.\n",
    "\n",
    "2. Aplicar t√©cnicas de imputaci√≥n, eliminaci√≥n o categorizaci√≥n\n",
    "para gestionar los valores nulos.\n",
    "\n",
    "3. Detectar outliers utilizando t√©cnicas como IQR y Z-score.\n",
    "\n",
    "4. Documentar las decisiones tomadas y c√≥mo impactan en la\n",
    "calidad del dataset.\n",
    "\n",
    "5. Guardar el DataFrame limpio para ser usado en la siguiente\n",
    "etapa.\n",
    "\n",
    "\n",
    "\n",
    "‚óè Lecci√≥n 5 - DATA WRANGLING\n",
    "\n",
    "üéØ Objetivo: Transformar y enriquecer los datos mediante t√©cnicas de\n",
    "manipulaci√≥n avanzada.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Tomar el DataFrame limpio de la Lecci√≥n 4.\n",
    "\n",
    "2. Aplicar t√©cnicas de Data Wrangling:\n",
    "\n",
    "‚óè Eliminar registros duplicados.\n",
    "‚óè Transformar tipos de datos.\n",
    "‚óè Crear nuevas columnas calculadas.\n",
    "‚óè Aplicar funciones personalizadas (apply(), map(),\n",
    "lambda).\n",
    "‚óè Normalizar o discretizar columnas seg√∫n sea necesario.\n",
    "\n",
    "3. Guardar la nueva versi√≥n del DataFrame optimizado.\n",
    "\n",
    "‚óè Lecci√≥n 6 - Agrupamiento y pivoteo de datos\n",
    "\n",
    "üéØ Objetivo: Organizar y estructurar los datos para el an√°lisis\n",
    "utilizando t√©cnicas de agrupamiento y pivotado.\n",
    "üìç Tareas a desarrollar:\n",
    "1. Tomar el DataFrame final de la Lecci√≥n 5.\n",
    "2. Aplicar t√©cnicas de agrupamiento (groupby()) para obtener\n",
    "m√©tricas resumidas.\n",
    "3. Reestructurar los datos utilizando pivot() y melt().\n",
    "4. Combinar nuevas fuentes de ser necesario con merge() y\n",
    "concat().\n",
    "5. Exportar el DataFrame final listo para an√°lisis en formatos\n",
    "CSV y Excel.\n",
    "6. Elaborar un documento resumen explicando todo el flujo de\n",
    "trabajo realizado, desde la Lecci√≥n 1 hasta la Lecci√≥n 6.\n",
    "\n",
    "¬øQu√© vamos a validar? üîç\n",
    "Aspectos t√©cnicos:\n",
    "‚Üí Correcto uso de las librer√≠as NumPy y Pandas.\n",
    "‚Üí Legibilidad, modularizaci√≥n y organizaci√≥n del c√≥digo.\n",
    "‚Üí Aplicaci√≥n correcta de las t√©cnicas vistas: imputaci√≥n, detecci√≥n de\n",
    "outliers, wrangling, agrupamiento y combinaci√≥n de datos.\n",
    "‚Üí Exportaci√≥n del dataset limpio y transformado.\n",
    "\n",
    "\n",
    "Aspectos estructurales:\n",
    "\n",
    "‚Üí Cumplimiento de todos los requerimientos generales y espec√≠ficos.\n",
    "‚Üí Documentaci√≥n clara y detallada que explique cada etapa del proceso.\n",
    "‚Üí Calidad de la estructura final del dataset.\n",
    "Aspectos de performance:\n",
    "‚Üí Correcta gesti√≥n del tiempo en la resoluci√≥n del proyecto.\n",
    "‚Üí Claridad en la presentaci√≥n y entrega de los resultados.\n",
    "‚Üí Capacidad de resoluci√≥n de problemas encontrados durante el tratamiento\n",
    "de los datos.\n",
    "Referencias ü¶∫\n",
    "‚Üí Documentaci√≥n oficial de NumPy: https://numpy.org/doc/\n",
    "‚Üí Documentaci√≥n oficial de Pandas: https://pandas.pydata.org/docs/\n",
    "Recursos üéÅ\n",
    "‚Üí https://github.com/pandas-dev/pandas\n",
    "‚Üí clientes_ecommerce.csv\n",
    "‚Üí clientes_ecommerce.xlsx\n",
    "\n",
    "\n",
    "Entregables ‚úÖ\n",
    "\n",
    "Al finalizar el proyecto \"Preparaci√≥n de Datos con Python\", se espera un\n",
    "consolidado integral final de lo planteado en cada una de las etapas (lecciones),\n",
    "como evidencia concreta del trabajo realizado:\n",
    "\n",
    "‚úÖ Entregable Final (Proyecto Integrador):\n",
    "\n",
    "1. Script o Notebook Python (.py o .ipynb)\n",
    "\n",
    "‚óè C√≥digo funcional y modularizado que integre todo el flujo de trabajo, desde\n",
    "la generaci√≥n de datos con NumPy hasta la exportaci√≥n final del dataset\n",
    "procesado.\n",
    "\n",
    "‚óè Puede estar dividido por secciones/lecciones o en bloques bien\n",
    "documentados dentro de un √∫nico archivo.\n",
    "\n",
    "2. Dataset final estructurado\n",
    "\n",
    "‚óè Archivos exportados en formato CSV y Excel que contengan el dataset\n",
    "limpio, transformado y preparado para an√°lisis o uso en modelos.\n",
    "\n",
    "‚óè Debe reflejar los pasos de imputaci√≥n, detecci√≥n de outliers, wrangling,\n",
    "agrupamientos y combinaciones realizados.\n",
    "\n",
    "3. Documento resumen del flujo de trabajo (PDF o Markdown)\n",
    "\n",
    "‚óè Un documento claro y conciso que explique el proceso completo seguido\n",
    "en el proyecto:\n",
    "\n",
    "‚óã Justificaci√≥n del uso de NumPy y Pandas.\n",
    "‚óã Descripci√≥n del dataset generado y de las fuentes externas\n",
    "integradas.\n",
    "‚óã T√©cnicas aplicadas para la limpieza y transformaci√≥n.\n",
    "‚óã Principales decisiones tomadas y desaf√≠os encontrados.\n",
    "‚óã Resultados obtenidos y estado final del dataset.\n",
    "\n",
    "Portafolio üíº\n",
    "\n",
    "El proyecto \"Preparaci√≥n de datos\" podr√° ser incorporado en tu portafolio\n",
    "profesional como un caso pr√°ctico de manipulaci√≥n y preparaci√≥n de datos\n",
    "reales. Te recomendamos presentar el proyecto destacando:\n",
    "\n",
    "‚óè Las t√©cnicas aplicadas y librer√≠as utilizadas.\n",
    "‚óè Las principales problem√°ticas encontradas y c√≥mo fueron resueltas.\n",
    "‚óè Capturas del c√≥digo, el dataset limpio y las visualizaciones de los\n",
    "resultados.\n",
    "\n",
    "Esto demostrar√° tu capacidad para abordar proyectos de preparaci√≥n de datos,\n",
    "una de las habilidades m√°s demandadas en la industria de datos y tecnolog√≠a.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faabb56",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14feb5d9",
   "metadata": {},
   "source": [
    "## Lecci√≥n 1 - La librer√≠a numpy\n",
    "\n",
    "üéØ Objetivo: Crear un conjunto de datos ficticio utilizando NumPy,\n",
    "aplicando operaciones b√°sicas para la preparaci√≥n inicial.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Crear un archivo .py o un Notebook .ipynb.\n",
    "\n",
    "2. Generar datos ficticios de clientes y transacciones utilizando\n",
    "arrays de NumPy.\n",
    "\n",
    "3. Aplicar operaciones matem√°ticas b√°sicas (suma, media,\n",
    "conteo, etc.).\n",
    "\n",
    "4. Guardar los datos generados en un archivo .npy o convertirlos\n",
    "a listas para usarlos luego en Pandas.\n",
    "\n",
    "5. Explicar en un breve documento por qu√© NumPy es eficiente\n",
    "para el manejo de datos num√©ricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear un archivo .py o un Notebook .ipynb.\n",
    "# crearemos un aprchivo ipynb por lo que no es necesario este paso en el c√≥digo.\n",
    "# elejiremos un archivo .ipynb para facilitar la visualizaci√≥n de los datos generados.\n",
    "\n",
    "# Importar la biblioteca NumPy.\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Generar datos ficticios de clientes y transacciones utilizando arrays de NumPy. Semilla(42) para reproducibilidad.\n",
    "np.random.seed(42)\n",
    "\n",
    "num_clientes = 1000\n",
    "clientes_ids = np.arange(1, num_clientes + 1)\n",
    "transacciones = np.random.randint(1, 1000, size=num_clientes)\n",
    "# Crear un array estructurado para almacenar los datos de clientes y transacciones\n",
    "datos = np.zeros(num_clientes, dtype=[('cliente_id', 'i4'), ('transaccion', 'i4')])\n",
    "datos['cliente_id'] = clientes_ids\n",
    "datos['transaccion'] = transacciones\n",
    "\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aplicar operaciones matem√°ticas b√°sicas (suma, media,conteo, etc.).\n",
    "total_transacciones = np.sum(datos['transaccion'])\n",
    "media_transacciones = np.mean(datos['transaccion'])\n",
    "conteo_transacciones = np.size(datos['transaccion'])\n",
    "print(f'Total de transacciones: {total_transacciones}')\n",
    "print(f'Media de transacciones: {media_transacciones}')\n",
    "print(f'Conteo de transacciones: {conteo_transacciones}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Guardar los datos generados en un archivo .npy o convertirlos a listas para usarlos luego en Pandas.\n",
    "np.save('datos_clientes_transacciones.npy', datos)\n",
    "\n",
    "# cargar datos desde el archivo .npy\n",
    "datos_cargados = np.load('datos_clientes_transacciones.npy')\n",
    "print(datos_cargados)  # Mostrar los datos cargados\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Explicar en un breve documento por qu√© NumPy es eficiente para el manejo de datos num√©ricos.\n",
    "\n",
    "\n",
    "# NumPy es eficiente para el manejo de datos num√©ricos debido a varias razones:\n",
    "\n",
    "# a. Estructuras de datos optimizadas: NumPy utiliza arrays multidimensionales\n",
    "# que son m√°s eficientes en t√©rminos de memoria y velocidad en comparaci√≥n con las listas\n",
    "# nativas de Python.\n",
    "\n",
    "# b. Operaciones vectorizadas: NumPy permite realizar operaciones matem√°ticas\n",
    "# en arrays completos sin necesidad de bucles expl√≠citos, lo que mejora significativamente\n",
    "# el rendimiento.\n",
    "\n",
    "# c. Implementaci√≥n en C: Muchas de las operaciones de NumPy est√°n implementadas\n",
    "# en C, lo que permite una ejecuci√≥n m√°s r√°pida en comparaci√≥n con el c√≥digo Python puro.\n",
    "\n",
    "# d. Amplia funcionalidad: NumPy ofrece una amplia gama de funciones matem√°ticas\n",
    "# y estad√≠sticas optimizadas para trabajar con datos num√©ricos de manera eficiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758bfd5",
   "metadata": {},
   "source": [
    "\n",
    "## Lecci√≥n 2 - La librer√≠a pandas\n",
    "\n",
    "üéØ Objetivo: Explorar y transformar los datos generados en la Lecci√≥n\n",
    "1, utilizando la estructura de DataFrame de Pandas.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Leer los datos preparados en NumPy y convertirlos en un\n",
    "DataFrame.\n",
    "\n",
    "2. Realizar una exploraci√≥n inicial:\n",
    "\n",
    "‚óè Visualizar primeras y √∫ltimas filas.\n",
    "\n",
    "‚óè Obtener estad√≠sticas descriptivas.\n",
    "\n",
    "‚óè Aplicar filtros condicionales.\n",
    "\n",
    "\n",
    "3. Guardar el DataFrame preliminar en un archivo CSV para ser\n",
    "utilizado en la siguiente lecci√≥n.\n",
    "\n",
    "4. Redactar un documento breve describiendo los hallazgos y la\n",
    "utilidad de Pandas para la manipulaci√≥n de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Leer los datos preparados en NumPy y convertirlos en un DataFrame.\n",
    "import pandas as pd\n",
    "datos_df = pd.DataFrame(datos)\n",
    "print(datos_df.head())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Realizar una exploraci√≥n inicial:\n",
    "# Visualizar primeras y √∫ltimas filas.\n",
    "print(\"Primeras filas del DataFrame:\")\n",
    "print(datos_df.head())\n",
    "print(\"√öltimas filas del DataFrame:\")\n",
    "print(datos_df.tail()) \n",
    "# Obtener estad√≠sticas descriptivas.\n",
    "print(\"Estad√≠sticas descriptivas del DataFrame:\")\n",
    "print(datos_df.describe())\n",
    "# Aplicar filtros condicionales.\n",
    "filtro = datos_df['transaccion'] > 500\n",
    "datos_filtrados = datos_df[filtro]\n",
    "print(\"Datos filtrados (transacciones > 500):\")\n",
    "print(datos_filtrados)  \n",
    "# Identificar valores nulos o duplicados.\n",
    "print(\"Valores nulos en el DataFrame:\")\n",
    "print(datos_df.isnull().sum())\n",
    "print(\"Filas duplicadas en el DataFrame:\")\n",
    "print(datos_df.duplicated().sum())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Guardar el DataFrame preliminar en un archivo CSV\n",
    "datos_df.to_csv('Ventas3_Autogenerado.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Redactar un documento breve describiendo los hallazgos y la utilidad de Pandas para la manipulaci√≥n de datos.\n",
    "\n",
    "# Pandas es una biblioteca poderosa y vers√°til para la manipulaci√≥n y an√°lisis de datos en Python.\n",
    "# Permite trabajar con estructuras de datos como DataFrames y Series, facilitando la limpieza,\n",
    "# transformaci√≥n y an√°lisis de datos. Con Pandas, es sencillo realizar operaciones como filtrado,\n",
    "# agrupamiento, agregaci√≥n y visualizaci√≥n de datos. Adem√°s, Pandas ofrece una integraci√≥n\n",
    "# fluida con otras bibliotecas de an√°lisis de datos y visualizaci√≥n, lo que la convierte en una\n",
    "# herramienta esencial para cient√≠ficos de datos y analistas. Su capacidad para manejar grandes\n",
    "# conjuntos de datos de manera eficiente y su sintaxis intuitiva hacen que Pandas sea una opci√≥n\n",
    "# preferida para la manipulaci√≥n de datos en el ecosistema de Python.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a530021",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Lecci√≥n 3 - Obtenci√≥n de datos desde archivos\n",
    "\n",
    "üéØ Objetivo: Integrar datos de diversas fuentes y unificarlos en un solo\n",
    "DataFrame para su posterior limpieza.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Cargar el archivo CSV generado en la Lecci√≥n 2.\n",
    "\n",
    "2. Incorporar nuevas fuentes de datos:\n",
    "\n",
    "‚óè Leer un archivo Excel con informaci√≥n complementaria.\n",
    "‚óè Extraer datos de una tabla web usando read_html().\n",
    "\n",
    "3. Unificar las diferentes fuentes de datos en un √∫nico DataFrame.\n",
    "\n",
    "4. Guardar el DataFrame consolidado y documentar los desaf√≠os\n",
    "encontrados al obtener datos de distintos formatos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Cargar el archivo CSV .\n",
    "import pandas as pd\n",
    "datos_df = pd.read_csv('Ventas2_CSV.csv')\n",
    "print(datos_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Incorporar nuevas fuentes de datos:\n",
    "# Leer un archivo Excel con informaci√≥n complementaria.\n",
    "# Extraer datos de una tabla web usando read_html().\n",
    "import pandas as pd\n",
    "# Leer archivo Excel\n",
    "excel_df = pd.read_excel('datos_complementarios.xlsx')\n",
    "print(\"Datos del archivo Excel:\")\n",
    "print(excel_df.head())\n",
    "# Leer tabla web\n",
    "url = 'https://example.com/tabla_datos'\n",
    "tablas = pd.read_html(url)\n",
    "web_df = tablas[0]\n",
    "print(\"Datos extra√≠dos de la tabla web:\")\n",
    "print(web_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Unificar las diferentes fuentes de datos en un √∫nico DataFrame.\n",
    "datos_completos_df = pd.merge(datos_df, excel_df, on='cliente_id', how='left')\n",
    "datos_completos_df = pd.merge(datos_completos_df, web_df, on='cliente_id', how='left')\n",
    "print(\"DataFrame unificado con todas las fuentes de datos:\")\n",
    "print(datos_completos_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Guardar el DataFrame consolidado y documentar los desaf√≠os encontrados al obtener datos de distintos formatos.\n",
    "# Guardar el DataFrame consolidado en un archivo CSV.\n",
    "datos_completos_df.to_csv('datos_completos_clientes.csv', index=False)\n",
    "\n",
    "# Desaf√≠os encontrados:\n",
    "# a. Formatos inconsistentes: Los datos provenientes de diferentes fuentes\n",
    "# pueden tener formatos distintos (por ejemplo, fechas en diferentes formatos,\n",
    "# nombres de columnas diferentes, etc.), lo que requiere limpieza y estandarizaci√≥n.\n",
    "# b. Valores faltantes: Al combinar datos de distintas fuentes,\n",
    "# es com√∫n encontrar valores faltantes que deben ser manejados adecuadamente.\n",
    "# c. Tipos de datos diferentes: Es posible que las mismas columnas\n",
    "# tengan tipos de datos diferentes en cada fuente, lo que puede causar problemas\n",
    "# al intentar unificarlos.\n",
    "# d. Rendimiento: La manipulaci√≥n de grandes conjuntos de datos\n",
    "# puede ser lenta y requerir optimizaciones para mejorar el rendimiento.\n",
    "# e. Dependencia de fuentes externas: Al extraer datos de la web,\n",
    "# la disponibilidad y estructura de la p√°gina pueden cambiar, afectando\n",
    "# la capacidad de obtener los datos correctamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a172104",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Lecci√≥n 4 - Manejo de valores perdidos y outliers\n",
    "\n",
    "üéØ Objetivo: Aplicar t√©cnicas de limpieza de datos, resolviendo\n",
    "problemas de valores nulos y datos at√≠picos.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Identificar valores nulos en el DataFrame consolidado.\n",
    "\n",
    "2. Aplicar t√©cnicas de imputaci√≥n, eliminaci√≥n o categorizaci√≥n\n",
    "para gestionar los valores nulos.\n",
    "\n",
    "3. Detectar outliers utilizando t√©cnicas como IQR y Z-score.\n",
    "\n",
    "4. Documentar las decisiones tomadas y c√≥mo impactan en la\n",
    "calidad del dataset.\n",
    "\n",
    "5. Guardar el DataFrame limpio para ser usado en la siguiente\n",
    "etapa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5065a08",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Lecci√≥n 5 - DATA WRANGLING\n",
    "\n",
    "üéØ Objetivo: Transformar y enriquecer los datos mediante t√©cnicas de\n",
    "manipulaci√≥n avanzada.\n",
    "\n",
    "üìç Tareas a desarrollar:\n",
    "\n",
    "1. Tomar el DataFrame limpio de la Lecci√≥n 4.\n",
    "\n",
    "2. Aplicar t√©cnicas de Data Wrangling:\n",
    "\n",
    "‚óè Eliminar registros duplicados.\n",
    "‚óè Transformar tipos de datos.\n",
    "‚óè Crear nuevas columnas calculadas.\n",
    "‚óè Aplicar funciones personalizadas (apply(), map(),\n",
    "lambda).\n",
    "‚óè Normalizar o discretizar columnas seg√∫n sea necesario.\n",
    "\n",
    "3. Guardar la nueva versi√≥n del DataFrame optimizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677a5f0",
   "metadata": {},
   "source": [
    "\n",
    "## Lecci√≥n 6 - Agrupamiento y pivoteo de datos\n",
    "\n",
    "üéØ Objetivo: Organizar y estructurar los datos para el an√°lisis\n",
    "utilizando t√©cnicas de agrupamiento y pivotado.\n",
    "üìç Tareas a desarrollar:\n",
    "1. Tomar el DataFrame final de la Lecci√≥n 5.\n",
    "2. Aplicar t√©cnicas de agrupamiento (groupby()) para obtener\n",
    "m√©tricas resumidas.\n",
    "3. Reestructurar los datos utilizando pivot() y melt().\n",
    "4. Combinar nuevas fuentes de ser necesario con merge() y\n",
    "concat().\n",
    "5. Exportar el DataFrame final listo para an√°lisis en formatos\n",
    "CSV y Excel.\n",
    "6. Elaborar un documento resumen explicando todo el flujo de\n",
    "trabajo realizado, desde la Lecci√≥n 1 hasta la Lecci√≥n 6.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
